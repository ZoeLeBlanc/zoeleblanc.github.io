<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://zoeleblanc.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://zoeleblanc.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-14T16:22:31+00:00</updated><id>https://zoeleblanc.github.io/feed.xml</id><title type="html">blank</title><subtitle>Zoe LeBlanc, Digital Historian. </subtitle><entry><title type="html">Programming Over Projects: Teaching Machine Learning for Humanities at an iSchool</title><link href="https://zoeleblanc.github.io/blog/2024/programming-over-projects/" rel="alternate" type="text/html" title="Programming Over Projects: Teaching Machine Learning for Humanities at an iSchool"/><published>2024-08-05T00:00:00+00:00</published><updated>2024-08-05T00:00:00+00:00</updated><id>https://zoeleblanc.github.io/blog/2024/programming-over-projects</id><content type="html" xml:base="https://zoeleblanc.github.io/blog/2024/programming-over-projects/"><![CDATA[<p>For DH2024, I was very lucky to be asked to join a workshop on Teaching Machine Learning for Humanists, and while I‚Äôm not sure I‚Äôll make it on time (currently waiting to see if my 6am flight will be delayed), I wanted to share my slides and notes here.</p> <h2 id="workshop-description">Workshop Description</h2> <p>The workshop was organized by Natalia Ermolaev and Andrew Janco, and I‚Äôm scheduled to give talks with Toma Tasovac, Melanie Walsh, Quinn Dombrowski, William Mattingly, Mia Ridge, Nathan Kelber, and Kahyun Choi. For background, this is the summary of the workshop:</p> <p>In recent years, advancements in machine learning (ML) have opened exciting new capabilities for the computational humanities, giving rise to new pedagogical initiatives to teach applied ML specifically in a DH context.</p> <p>This workshop will meet at DH2024: Reinvention and Responsibility to share experiences, best practices, and strategies for teaching ML‚Äîits techniques, potentials, and risks‚Äîto the humanities community.</p> <p>The presenters and participants of this workshop teach (or plan to teach) ML to humanists at multiple levels‚Äîundergraduates, graduate students, as well as faculty and advanced researchers‚Äîand in various modalities‚Äîin-person, hybrid, and asynchronous.</p> <p>Attendance is open to any technologist, researcher, librarian or student interested in either refining their methodology of teaching ML in the humanities context, or is curious to embark on teaching ML to humanists.</p> <p>And here‚Äôs a fun photo from the WHOVA app for promoting the workshop:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DH2024_workshop-480.webp 480w,/assets/img/DH2024_workshop-800.webp 800w,/assets/img/DH2024_workshop-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DH2024_workshop.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="programming-over-projects">Programming over Projects</h2> <p>Here‚Äôs the text of my talk, I‚Äôll update with the slides and video when I can and you can find links to all my syllabi on my <a href="/teaching/">teaching page</a>.</p> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DH2024_workshop/Slide1-480.webp 480w,/assets/img/DH2024_workshop/Slide1-800.webp 800w,/assets/img/DH2024_workshop/Slide1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DH2024_workshop/Slide1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Hi everyone! I‚Äôm thrilled to be part of this workshop, and want to start by thanking both the workshop and conference organizers for all their hard work üëèüèΩ!</p> <p>For those I haven‚Äôt met before, I‚Äôm Zoe LeBlanc and I‚Äôm an Assistant Professor at the School of Information Sciences at the University of Illinois, Urbana-Champaign. My research focuses on histories of information and digital humanities, and I also teach many of the DH courses at the iSchool.</p> <p>Today, I‚Äôm excited to share my experiences teaching machine learning, particularly for students who, even though they‚Äôre in an iSchool, have a strong interest in humanities research and are eager to explore how these methods can enhance their work.</p> <p>Since I only have seven minutes, I‚Äôll be focusing on two courses that I‚Äôve developed. The first course is aimed at advanced iSchool graduate students, though it also attracts doctoral students from other departments (mainly Media &amp; Cinema Studies so far), as well as some advanced MSLIS students. The second course is part of our relatively new undergraduate program. Unlike most humanities departments, our iSchool only launched its undergraduate program about four years ago, so we‚Äôre still figuring out how to best teach these methods to students who are new to the field.</p> <h3 id="culture-at-scale">Culture At Scale</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DH2024_workshop/Slide2-480.webp 480w,/assets/img/DH2024_workshop/Slide2-800.webp 800w,/assets/img/DH2024_workshop/Slide2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DH2024_workshop/Slide2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The first course I‚Äôm going to talk about is called <em>Culture at Scale</em>. The title might sound a bit unusual, but I wanted to capture the essence of how scale and computational methods can help us study culture. This course is something I had been dreaming about for years‚Äì‚Äìhonestly, ever since I was a PhD student‚Äì‚Äìso I was very excited to finally put it together, and I‚Äôve taught it twice now.</p> <p><em>Culture at Scale</em> is a unique course, I would wager, in DH (or at least American DH). Unlike many DH courses that serve as an introduction to programming, this is not an entry-level course. Instead, it‚Äôs intended for students who have already completed at least two or three courses that are quite intensive in programming and statistics, sometimes even more. For example, last semester, many of the students had taken an advanced graduate seminar on LLMs. This means that for this group of students, knowing how to code is usually not the issue and they are generally aware of at least some advanced computational methods.</p> <p>This course is also a bit unique in that given my iSchool has a longstanding investment in the Humanities, unlike some other institutions. So, many of these students already have a deep interest in humanities research, often coming from backgrounds in English, History, Art, etc. This means that I‚Äôm not hampered by the usual obstacles to DH teaching, such as a lack of programming knowledge or an understanding of what constitutes an interesting humanities research question.</p> <p>But even with this structural support, I‚Äôll be frank that this course has been challenging to teach, since its focus goes far beyond any one discipline (my Zotero library for the course is massive!).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DH2024_workshop/Slide3-480.webp 480w,/assets/img/DH2024_workshop/Slide3-800.webp 800w,/assets/img/DH2024_workshop/Slide3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DH2024_workshop/Slide3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>To help manage this, I‚Äôve broken the course into three themes:</p> <p>The first theme is the experience of conducting a research project centered on <em>Culture at Scale</em>, which is the core assignment but also one of the discussion themes of the course. Essentially, digging into how we do this work. We explore what sorts of research questions are amenable to this type of work and how we interpret the results, as well as discussing the various stages of this type of work (from struggling to find data to struggling to get it correctly formatted to struggling to interpret it‚Äì‚Äìlots of struggles!). The goal is to help students navigate a hands-on experience of applying computational methods to cultural data.</p> <p>The second theme is the historical development of <em>Culture at Scale</em>. We examine how computation has historically been used to study humanities questions. This includes looking at how different fields and disciplines have approached this work, as well as the criticisms that have emerged both from within and outside these fields. This historical perspective helps students understand the evolution and the interdisciplinary nature of computational humanities.</p> <p>The third theme focuses on evaluating the methodological and theoretical trends in working with <em>Culture at Scale</em>. We assess the state-of-the-art practices and consider whether we can view these diverse practices holistically, despite their origins in different computational fields and disciplines. This theme encourages students to critically engage with current methodologies and to think about the theoretical implications of their work.</p> <h4 id="ml--culture-at-scale">ML &amp; Culture At Scale</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DH2024_workshop/Slide4-480.webp 480w,/assets/img/DH2024_workshop/Slide4-800.webp 800w,/assets/img/DH2024_workshop/Slide4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DH2024_workshop/Slide4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>When it comes to machine learning specifically, I have two main goals in the course.</p> <p>The first goal is to situate machine learning within a broader computational context. This means viewing machine learning as part of a continuum that includes descriptive exploratory statistics, network science, and the often-discussed divide between unsupervised and supervised learning, to name but a few methods that we discuss. I want students to see these methods as interconnected approaches to working with cultural data, rather than isolated techniques. This holistic view is crucial because in DH, there is a tendency to adopt methods based on their historical popularity rather than their suitability for specific research questions.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> In the course, students must justify their choice of methods, arguing why one approach is better suited to their research than another, but also consider how working with texts or images or other forms of data often can benefit from a combination of similar methods.</p> <p>The second goal is to critically examine what constitutes best practices in this evolving field. Much of our work in DH draws on practices from other fields that have different assumptions about data and its representation. For instance, how we handle missing data can differ significantly from other disciplines. We need to develop best practices that are informed by humanistic concerns. This means placing less emphasis on memorizing equations or chasing the latest state-of-the-art technique for its own sake. Instead, we focus on what it means to report metrics like F1 scores or resample when dealing with historical data. We also emphasize the importance of experimenting with hyper-parameters and validation measurements to understand the stability of the models and methods we use for our research questions.</p> <p>Ultimately, there is a significant debate in DH regarding the role of computation. I try to be fairly agnostic on this debate, but in the course I do try to encourage students to explore computation holistically, from ‚Äòstirring the archive‚Äô to modeling it. My hope is that this helps them think critically about how and where they want to utilize computation to further their knowledge claims.</p> <h3 id="computing-and-the-humanitiesculture-as-data">Computing and the Humanities/Culture as Data</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DH2024_workshop/Slide5-480.webp 480w,/assets/img/DH2024_workshop/Slide5-800.webp 800w,/assets/img/DH2024_workshop/Slide5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DH2024_workshop/Slide5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Part of what makes Culture At Scale doable is that there‚Äôs a strong emphasis on project-based learning (students propose a project and work on it over the course of the semester), which is something that I‚Äôve also integrated into my undergraduate course <em>Computing in the Humanities</em>.</p> <p>Now I want to emphasize that I know I‚Äôm incredibly lucky to have classes small enough to make project-based learning feasible. I also know that this is not the case for many DH courses, especially at the undergraduate level. But I do think that project-based learning is crucial for teaching machine learning in the humanities. It‚Äôs not enough to just learn the theory or the methods; students need to apply these methods to real-world problems to understand their strengths and limitations.</p> <p>This is especially challenging with undergraduates in our program since almost none of them have any humanities experience. In fact, most are so unfamiliar with the term that I‚Äôm trying to get the course renamed to <em>Culture as Data</em> to better reflect the content. The focus of the course is on understanding how and why we represent culture as data, helping students understand that data, rather than being a given, is actually constructed.</p> <h4 id="ml--computing-in-the-humanities">ML &amp; Computing in the Humanities</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DH2024_workshop/Slide6-480.webp 480w,/assets/img/DH2024_workshop/Slide6-800.webp 800w,/assets/img/DH2024_workshop/Slide6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DH2024_workshop/Slide6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>A key component of this course is understanding data curation holistically. Students are tasked with creating data in some way‚Äî‚Äì‚Äìeither by finding and combining existing datasets or creating new ones from scratch. One of the difficulties in creating data is that students often don‚Äôt understand the impacts of their choices until they actually use the data. To address this, I introduce them to basic clustering through topic modeling, classification through TF-IDF and Logistic Regression, as well as the general principles of text analysis. We also touch on network analysis to help them see how their data representation choices determine the utility of these methods. While I‚Äôm definitely covering a lot in this course, I do think including these methods is crucial to helping students understand how they open up new ways of interpreting cultural data, even if the students don‚Äôt have much of a sense of what makes a good research question or not.</p> <p>This is also where project-based learning is crucial since students in the course often have a diverse background when it comes to programming and computational methods, so working on a project allows them to experiment without having the pressure to produce a perfect result. This is especially important when it comes to machine learning, where the results can be quite unstable and the methods can be quite opaque.</p> <p>Similar to Culture at Scale, machine learning is not the central focus of the course. We don‚Äôt spend a lot of time optimizing models; instead, we try out different methods, test hyper-parameters and assess the stability of the results, and finally explore how the methods and data affect the outcomes. In essence, I try to approach machine learning as an applied method for humanities research. This means that students need to understand the assumptions built into these methods and how they affect the results. This is a significant shift from how machine learning is often taught, where the focus is on the technical aspects of the method rather than the interpretation of the results.</p> <h3 id="trade-offs-and-takeaways">Trade-offs and Takeaways</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DH2024_workshop/Slide7-480.webp 480w,/assets/img/DH2024_workshop/Slide7-800.webp 800w,/assets/img/DH2024_workshop/Slide7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DH2024_workshop/Slide7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>To sum up, I‚Äôd like to discuss some of the trade-offs, takeaways, struggles, and successes I‚Äôve experienced. As you can tell, I prioritize a contextual understanding of machine learning‚Äîhow we interpret the results‚Äîover a purely technical understanding. It‚Äôs important for students to grasp the assumptions built into these methods, but I‚Äôm more concerned with helping them see the applicability of these methods to studying culture.</p> <p>There are significant trade-offs though, especially in getting students up and running with the necessary technical infrastructure. Many students struggle with installing libraries, setting up virtual environments, and understanding the larger coding infrastructure. Moreover, students without a humanities background often find it challenging to determine what makes interesting results using machine learning. I try to manage this through project-based learning, but it remains a struggle.</p> <p>One of the successes is that this approach benefits from a robust curriculum, which is rare for DH courses that tend to be one-offs. Instead, this is not the only course where students encounter machine learning. We have courses specifically focused on machine learning; though those often treat data as a case study for the method, whereas my course inverts this relationship. This allows students to see the potential value of this work and decide if they want to pursue it further. Even if they choose not to, they gain the ability to critically evaluate machine learning and understand both its strengths and limitations.</p> <p>In my courses, I foreground interpretation rather than methods. Such an approach is far from perfect, but I think it provides an important contrast with how CS typically handles these topics and, I believe, provides a valuable and necessary perspective for students. Ultimately, I think my approach can be summarized as prioritizing <strong>projects over programming</strong>. Despite my love for programming (including ML), I‚Äôve found that this project-based focus is the most effective for students looking to understand how computation can help us study culture.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DH2024_workshop/Slide8-480.webp 480w,/assets/img/DH2024_workshop/Slide8-800.webp 800w,/assets/img/DH2024_workshop/Slide8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DH2024_workshop/Slide8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Thanks for listening, and I‚Äôm happy to take any questions!</p> <hr/> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>I‚Äôm not trying to call anyone out here, but do think there‚Äôs some deep structural issues in American DH that prevents us from thinking critically about the methods we use. I‚Äôm happy to talk more about this in the Q&amp;A or come to my panel on Cultural Analytics Research and Teaching Friday morning!¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[A talk from the Teaching ML for Humanities Workshop at DH2024]]></summary></entry><entry><title type="html">Why an iSchool</title><link href="https://zoeleblanc.github.io/blog/2021/why-ischool/" rel="alternate" type="text/html" title="Why an iSchool"/><published>2021-10-10T00:00:00+00:00</published><updated>2021-10-10T00:00:00+00:00</updated><id>https://zoeleblanc.github.io/blog/2021/why-ischool</id><content type="html" xml:base="https://zoeleblanc.github.io/blog/2021/why-ischool/"><![CDATA[<p>If you know me, you know that I‚Äôm horrible at keeping things up to date. I tend to forget people might be curious as to what I‚Äôm up to, and also notoriously juggle too many projects, so things like emails or updating my bio get pushed to the side (also standing apology if I owe you an email!). Hence my CV posted here is from maybe 2017 and my linkedin is practically a ghost town. I‚Äôm trying to be more on top of these things, though I also realize that maybe this is just the type of person I am ‚Äì one who both struggles with day-to-day tasks and also if I‚Äôm being completely honest has an almost pathological need to keep most of my life private (not very millennial of me).</p> <p>One major update that I haven‚Äôt posted yet is that in August I officially joined the School of Information Sciences at the University of Illinois at Urbana-Champaign as an Assistant Professor.</p> <p>To be honest even writing that sentence still leaves me frozen with shock. Even though I‚Äôm already teaching and starting to plan future initiatives and join projects, it all still feels frankly surreal. I had for a long time assumed there was no spot in academia for someone like me - too in love with coding and computation for most history departments and too in love with thinking about how knowledge is produced for most CS departments. In the last year, I‚Äôve been asked to join quite a few career panels and that story probably deserves its own post but suffice to say I didn‚Äôt take the established path to end up here (in fact I didn‚Äôt even until a few years ago know this option existed).</p> <p>On one hand, it feels embarrassing a bit to admit that I wasn‚Äôt focused on this path since I was twelve since that seems to be the norm these days. But on the other hand, I increasingly think that maybe my lack of direction was emblematic of how until recently doing digital humanities work had to be outside of the normal established paths.</p> <p>So rather than go into a long deep dive about how I ended up here, this post is largely supposed to announce that I‚Äôm at UIUC and then encourage graduate students to apply. In fact, I meant to post this a few weeks back when my new colleague (though long time academic inspiration) Ryan Cordell‚Äôs posted this fantastic ‚ÄúWhy an iSchool?‚Äù https://ryancordell.org/research/why-iSchool/. Ryan details how he made his decision to join the iSchool and also why humanists might want to</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Life updates and other news]]></summary></entry><entry><title type="html">Guide to Publishing Humanities Data Analysis</title><link href="https://zoeleblanc.github.io/blog/2021/publish-altair/" rel="alternate" type="text/html" title="Guide to Publishing Humanities Data Analysis"/><published>2021-06-24T00:00:00+00:00</published><updated>2021-06-24T00:00:00+00:00</updated><id>https://zoeleblanc.github.io/blog/2021/publish-altair</id><content type="html" xml:base="https://zoeleblanc.github.io/blog/2021/publish-altair/"><![CDATA[<p>Now that you‚Äôre undertaking humanities data analysis a natural question is how to present and make your analyses accessible and publicly available. This is a BIG question with many answers depending on your tools, audiences, and timelines. However, for the sake of your final projects, I wanted to cover some of the options you might pursue to present your work.</p> <p>First though let‚Äôs refresh on how the web works.</p> <hr/> <h2 id="what-is-the-web">What is the Web?</h2> <p><a href="https://en.wikipedia.org/wiki/World_Wide_Web"><strong>From Wikipedia:</strong></a> ‚ÄúThe World Wide Web (WWW), commonly known as the Web, is an information system where documents and other web resources are identified by Uniform Resource Locators (URLs, such as https://www.example.com/), which may be interlinked by hypertext, and are accessible over the Internet.[1][2] The resources of the WWW are transferred via the Hypertext Transfer Protocol (HTTP) and may be accessed by users by a software application called a web browser and are published by a software application called a web server.‚Äù</p> <p><img src="https://2.bp.blogspot.com/_4l9wMe5bbSk/TMpvwVcMT3I/AAAAAAAAAK4/sCEjRQCkF1o/s1600/Client+Server+communication.GIF" alt="http"/></p> <p><em>What does this mean exactly?</em></p> <p>When we type a url for a webpage (say google.com), we‚Äôre actually sending an <strong>HTTP request</strong> to a <strong>server</strong> that hosts the actual HTML files and data. If the server decides that our request is ok (200), then it will give us access to the webpage.</p> <p><img src="https://d210waafu5nnsw.cloudfront.net/fad1ff76-801d-4adb-be70-9ddfacfeda88/images/u4ccf6dfb1090_original_opt.jpeg" alt="404 page"/></p> <p>If you‚Äôve ever seen an error message when you go to a webpage saying the page doesn‚Äôt exist, that means that you received a 404 error, which is a type of status code you get back from an HTTP request.</p> <p><em>So what does this mean for publishing on the web?</em></p> <p>To publish humanities data analysis requires having your files <strong>hosted</strong> on a server that are accessible by <strong>url</strong> - essentially you need a website.</p> <p>Most DH projects are hosted from Github, which is a service that allows you to host files and keep versions of those files (this is what I use for the course website.)</p> <p>If you decide you want to setup your own website, I would recommend following this tutorial on creating <a href="https://pages.github.com/">Github websites</a> and this book on using <a href="https://shane-et-al.github.io/git_slab/#git-and-github">git for DH projects</a>.</p> <hr/> <h2 id="publishing-jupyter-notebooks">Publishing Jupyter Notebooks</h2> <p>One of the easiest ways to publish your data analysis is to simply publish a Jupyter notebook. There‚Äôs a few ways you can go about publishing these depending on whether you want to embed the notebook in a website or simply have an accessible link.</p> <ol> <li>The simplest option is to upload your Jupyter notebook to <strong>Google Colab</strong> <a href="https://colab.research.google.com/notebooks/intro.ipynb#recent=true">https://colab.research.google.com/notebooks/intro.ipynb#recent=true</a> and <a href="https://colab.research.google.com/notebooks/welcome.ipynb">https://colab.research.google.com/notebooks/welcome.ipynb</a>. You can run your entire notebook in the Google Colab environment and even do your analysis there instead of running it locally.</li> <li>You can also use <strong>Binder</strong> <a href="https://mybinder.org/">https://mybinder.org/</a> and <a href="https://mybinder.readthedocs.io/en/latest/introduction.html">https://mybinder.readthedocs.io/en/latest/introduction.html</a> to host notebooks that you have in a Github repository.</li> <li>Lastly, you can convert a Jupyter notebook to HTML <a href="https://reproducible-science-curriculum.github.io/publication-RR-Jupyter/02-exporting_the_notebook/index.html">https://reproducible-science-curriculum.github.io/publication-RR-Jupyter/02-exporting_the_notebook/index.html</a> either in the terminal or through the web interface. You‚Äôll still need to host your html page, but this can be useful if you have a website and want to have the notebook as a page on the site.</li> </ol> <p>One concern with using Altair and Jupyter notebooks is that it tends to make the file very large since it loads the data into the notebook. The size of file can be a problem for Github, so if you are ending with very large files, check out these instructions from Altair‚Äôs documentation <a href="https://altair-viz.github.io/user_guide/faq.html#why-does-altair-lead-to-such-extremely-large-notebooks">https://altair-viz.github.io/user_guide/faq.html#why-does-altair-lead-to-such-extremely-large-notebooks</a></p> <hr/> <h2 id="publishing-altair-charts">Publishing Altair Charts</h2> <p>While publishing Jupyter notebooks might be an easier solution, often times you‚Äôll want a different reading experience than the cell format of a notebook. In that case, you can save your Altair charts and embed them on your website.</p> <p>For general guidelines on saving Altair charts, check out the documentation here <a href="https://altair-viz.github.io/user_guide/faq.html#why-does-altair-lead-to-such-extremely-large-notebooks">https://altair-viz.github.io/user_guide/saving_charts.html</a></p> <ol> <li>You can save your Altair chart directly to html with the <code class="language-plaintext highlighter-rouge">chart.save('chart.html')</code> syntax. You‚Äôll still need to host this html page on a website, but now you only have the chart rather than the entire notebook.</li> <li>The other option is to download the compiled JSON of the chart and embed it using JavaScript.</li> </ol> <p>You have the option to download the compiled json from your chart by clicking the <code class="language-plaintext highlighter-rouge">Open in Vega Editor</code>.</p> <div class="row" style="justify-content:center;"> <div> <img src="/assets/img/click.png" alt="Click Buttons on Altair Graph" style="max-width:100%;"/> </div> </div> <p>This will open a new window with the full Vega schema. You can select the export option, which will allow you to download the JSON (you can select either compiled or vega-lite).</p> <div class="row" style="justify-content:center;"> <div> <img src="/assets/img/export.png" alt="Exporting Altair" style="max-width:100%;"/> </div> </div> <p>The next step is to import this compiled json into your webpage.</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">&lt;!DOCTYPE html&gt;</span>
<span class="nt">&lt;html&gt;</span>
  <span class="nt">&lt;head&gt;</span>
    <span class="nt">&lt;title&gt;</span>Embedding Vega-Lite<span class="nt">&lt;/title&gt;</span>
    <span class="nt">&lt;script </span><span class="na">src=</span><span class="s">"https://cdn.jsdelivr.net/npm/vega@5.10.1"</span><span class="nt">&gt;&lt;/script&gt;</span>
    <span class="nt">&lt;script </span><span class="na">src=</span><span class="s">"https://cdn.jsdelivr.net/npm/vega-lite@4.10.4"</span><span class="nt">&gt;&lt;/script&gt;</span>
    <span class="nt">&lt;script </span><span class="na">src=</span><span class="s">"https://cdn.jsdelivr.net/npm/vega-embed@6.5.2"</span><span class="nt">&gt;&lt;/script&gt;</span>
  <span class="nt">&lt;/head&gt;</span>
  <span class="nt">&lt;body&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">"vis"</span><span class="nt">&gt;&lt;/div&gt;</span>

    <span class="nt">&lt;script </span><span class="na">type=</span><span class="s">"text/javascript"</span><span class="nt">&gt;</span>
      <span class="kd">var</span> <span class="nx">yourVlSpec</span> <span class="o">=</span> <span class="p">{</span>
        <span class="na">$schema</span><span class="p">:</span> <span class="dl">'</span><span class="s1">https://vega.github.io/schema/vega-lite/v2.0.json</span><span class="dl">'</span><span class="p">,</span>
        <span class="na">description</span><span class="p">:</span> <span class="dl">'</span><span class="s1">A simple bar chart with embedded data.</span><span class="dl">'</span><span class="p">,</span>
        <span class="na">data</span><span class="p">:</span> <span class="p">{</span>
          <span class="na">values</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span><span class="na">a</span><span class="p">:</span> <span class="dl">'</span><span class="s1">A</span><span class="dl">'</span><span class="p">,</span> <span class="na">b</span><span class="p">:</span> <span class="mi">28</span><span class="p">},</span>
            <span class="p">{</span><span class="na">a</span><span class="p">:</span> <span class="dl">'</span><span class="s1">B</span><span class="dl">'</span><span class="p">,</span> <span class="na">b</span><span class="p">:</span> <span class="mi">55</span><span class="p">},</span>
            <span class="p">{</span><span class="na">a</span><span class="p">:</span> <span class="dl">'</span><span class="s1">C</span><span class="dl">'</span><span class="p">,</span> <span class="na">b</span><span class="p">:</span> <span class="mi">43</span><span class="p">},</span>
            <span class="p">{</span><span class="na">a</span><span class="p">:</span> <span class="dl">'</span><span class="s1">D</span><span class="dl">'</span><span class="p">,</span> <span class="na">b</span><span class="p">:</span> <span class="mi">91</span><span class="p">},</span>
            <span class="p">{</span><span class="na">a</span><span class="p">:</span> <span class="dl">'</span><span class="s1">E</span><span class="dl">'</span><span class="p">,</span> <span class="na">b</span><span class="p">:</span> <span class="mi">81</span><span class="p">},</span>
            <span class="p">{</span><span class="na">a</span><span class="p">:</span> <span class="dl">'</span><span class="s1">F</span><span class="dl">'</span><span class="p">,</span> <span class="na">b</span><span class="p">:</span> <span class="mi">53</span><span class="p">},</span>
            <span class="p">{</span><span class="na">a</span><span class="p">:</span> <span class="dl">'</span><span class="s1">G</span><span class="dl">'</span><span class="p">,</span> <span class="na">b</span><span class="p">:</span> <span class="mi">19</span><span class="p">},</span>
            <span class="p">{</span><span class="na">a</span><span class="p">:</span> <span class="dl">'</span><span class="s1">H</span><span class="dl">'</span><span class="p">,</span> <span class="na">b</span><span class="p">:</span> <span class="mi">87</span><span class="p">},</span>
            <span class="p">{</span><span class="na">a</span><span class="p">:</span> <span class="dl">'</span><span class="s1">I</span><span class="dl">'</span><span class="p">,</span> <span class="na">b</span><span class="p">:</span> <span class="mi">52</span><span class="p">}</span>
          <span class="p">]</span>
        <span class="p">},</span>
        <span class="na">mark</span><span class="p">:</span> <span class="dl">'</span><span class="s1">bar</span><span class="dl">'</span><span class="p">,</span>
        <span class="na">encoding</span><span class="p">:</span> <span class="p">{</span>
          <span class="na">x</span><span class="p">:</span> <span class="p">{</span><span class="na">field</span><span class="p">:</span> <span class="dl">'</span><span class="s1">a</span><span class="dl">'</span><span class="p">,</span> <span class="na">type</span><span class="p">:</span> <span class="dl">'</span><span class="s1">ordinal</span><span class="dl">'</span><span class="p">},</span>
          <span class="na">y</span><span class="p">:</span> <span class="p">{</span><span class="na">field</span><span class="p">:</span> <span class="dl">'</span><span class="s1">b</span><span class="dl">'</span><span class="p">,</span> <span class="na">type</span><span class="p">:</span> <span class="dl">'</span><span class="s1">quantitative</span><span class="dl">'</span><span class="p">}</span>
        <span class="p">}</span>
      <span class="p">};</span>
      <span class="nx">vegaEmbed</span><span class="p">(</span><span class="dl">'</span><span class="s1">#vis</span><span class="dl">'</span><span class="p">,</span> <span class="nx">yourVlSpec</span><span class="p">);</span>
    <span class="nt">&lt;/script&gt;</span>
  <span class="nt">&lt;/body&gt;</span>
<span class="nt">&lt;/html&gt;</span>
</code></pre></div></div> <p>In this example from the Vega docs, <code class="language-plaintext highlighter-rouge">yourV1spec</code> is our compiled chart.</p> <p>So for our purposes we would have the following:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">&lt;!DOCTYPE html&gt;</span>
<span class="nt">&lt;html&gt;</span>
  <span class="nt">&lt;head&gt;</span>
    <span class="nt">&lt;title&gt;</span>Embedding Vega-Lite<span class="nt">&lt;/title&gt;</span>
    <span class="nt">&lt;script </span><span class="na">src=</span><span class="s">"https://cdn.jsdelivr.net/npm/vega@5.10.1"</span><span class="nt">&gt;&lt;/script&gt;</span>
    <span class="nt">&lt;script </span><span class="na">src=</span><span class="s">"https://cdn.jsdelivr.net/npm/vega-lite@4.10.4"</span><span class="nt">&gt;&lt;/script&gt;</span>
    <span class="nt">&lt;script </span><span class="na">src=</span><span class="s">"https://cdn.jsdelivr.net/npm/vega-embed@6.5.2"</span><span class="nt">&gt;&lt;/script&gt;</span>
  <span class="nt">&lt;/head&gt;</span>
  <span class="nt">&lt;body&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">"vis"</span><span class="nt">&gt;&lt;/div&gt;</span>

    <span class="nt">&lt;script </span><span class="na">type=</span><span class="s">"text/javascript"</span><span class="nt">&gt;</span>
      <span class="kd">var</span> <span class="nx">yourVlSpec</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">/visualization.vl.json</span><span class="dl">"</span><span class="p">;</span>
      <span class="nx">vegaEmbed</span><span class="p">(</span><span class="dl">'</span><span class="s1">#vis</span><span class="dl">'</span><span class="p">,</span> <span class="nx">yourVlSpec</span><span class="p">);</span>
    <span class="nt">&lt;/script&gt;</span>
  <span class="nt">&lt;/body&gt;</span>
<span class="nt">&lt;/html&gt;</span>
</code></pre></div></div> <p>Obviously we haven‚Äôt covered much about HTML and haven‚Äôt discussed at all JavaScript, but you might recognize some of the syntax inherently.</p> <p>Our <code class="language-plaintext highlighter-rouge">script</code> tags at the beginning of our webpage are importing the required libraries (just like our <code class="language-plaintext highlighter-rouge">import</code> statements).</p> <p>In our last <code class="language-plaintext highlighter-rouge">script</code> tag, we‚Äôre writing our code. First we are assigning a variable called <code class="language-plaintext highlighter-rouge">yourVlSpec</code> and passing it a string that contains the path to our compiled chart. Then we pass that variable to the class of <code class="language-plaintext highlighter-rouge">vegaEmbed</code>.</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// More argument info at https://github.com/vega/vega-embed
vegaEmbed('#vis', yourVlSpec);
</code></pre></div></div> <p>This class takes two arguments. The first is where we want the chart displayed (in this case a <code class="language-plaintext highlighter-rouge">div</code> with the <code class="language-plaintext highlighter-rouge">id</code> of <code class="language-plaintext highlighter-rouge">vis</code>). Then the second argument is the variable with the compiled chart.</p> <p>The benefit of this approach let‚Äôs you include other materials on your webpage, like additional text and images, as well as allowing different layouts.</p> <p>However, using this approach also requires quite a bit of additional work so you may want to start with publishing your notebooks before you start building an actual website.</p> <p>And finally here‚Äôs a live example!</p> <script src="https://cdn.jsdelivr.net/npm/vega@5.7.2"></script> <script src="https://cdn.jsdelivr.net/npm/vega-lite@4.0.0-beta.10"></script> <script src="https://cdn.jsdelivr.net/npm/vega-embed@5.1.3"></script> <div id="viz" class="col three"></div> <script type="text/javascript">var viz="/example_visualization.vl.json";vegaEmbed("#viz",viz);</script>]]></content><author><name></name></author><summary type="html"><![CDATA[Notes for publishing humanities data analysis graphs and some example code]]></summary></entry><entry><title type="html">NLP and DH</title><link href="https://zoeleblanc.github.io/blog/2020/nlp-and-dh/" rel="alternate" type="text/html" title="NLP and DH"/><published>2020-01-26T00:00:00+00:00</published><updated>2020-01-26T00:00:00+00:00</updated><id>https://zoeleblanc.github.io/blog/2020/nlp-and-dh</id><content type="html" xml:base="https://zoeleblanc.github.io/blog/2020/nlp-and-dh/"><![CDATA[<p>For Princeton‚Äôs Research Data Management Workshop for graduate students this Winter, I put together a more meta-level talk about natural language processing and digital humanities. The talk introduces a few tools and terms, but more broadly tries to introduce graduate students to the types of questions and processes that exist for doing this type of research. You can see the full text of the talk <a href="https://www.notion.so/Natural-Language-Processing-and-Digital-Humanities-6da7c69f6b0645f189a0d3a81481defb">here</a> and the slides are available <a href="https://docs.google.com/presentation/d/19p1puwByQgYgJALBxcjudxMWPguQrWRUBND0Ef8uzS4/edit?usp=sharing">here</a>.</p> <p>As always, I welcome any feedback and feel free to reuse my talk (though let me know since I would love to hear how it goes!)</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Notes and slides for recent talk]]></summary></entry><entry><title type="html">Generalizing Static Sites</title><link href="https://zoeleblanc.github.io/blog/2019/generalizing-static-sites/" rel="alternate" type="text/html" title="Generalizing Static Sites"/><published>2019-11-06T00:00:00+00:00</published><updated>2019-11-06T00:00:00+00:00</updated><id>https://zoeleblanc.github.io/blog/2019/generalizing-static-sites</id><content type="html" xml:base="https://zoeleblanc.github.io/blog/2019/generalizing-static-sites/"><![CDATA[<p>Recently, at the CDH we were discussing the possibility of creating a research periodical using a static site, and the question came up regarding which library to choose. In digital humanities, Jekyll seems to be by far the preferred choice, but beyond DH, there‚Äôs hundreds of options. Quickly, I realized that part of the issue for DHers is not so much actually getting a static site up an running, but understanding the larger landscape to decide which fits best for your project/problem.</p> <p>So I quickly put together a workshop with the idea of talking more about the conceptual issues and technical tradeoffs around static sites, rather than getting people to install another programming library (though I do think that‚Äôs the number one way you actually get a sense of static site libraries).</p> <p>You can find the initial workshop notes <a href="https://github.com/ZoeLeBlanc/static_site_workshop">here</a>, and I‚Äôll be updating them in the coming days to include a bit more in-depth explanation of static sites (namely what is HTML, CSS, and Javascript). There‚Äôs lots of links to resources and I tried to lay out the overall workflow of setting up a static site in using multiple libraries.</p> <p>Regardless, though I tried to focus on these broader questions and how you can answer them.</p> <ol> <li> <p>What programming language are you interested in using or learning? Most of these involve using HTML, CSS, and JavaScript, but some like GatsbyJS require some knowledge of React whereas Jekyll only requires a tiny bit of Ruby to work in the Makefile</p> </li> <li> <p>How much complexity will your site have? How many pages? How many different types of content? Some static site generators and themes make assumptions for how your content is organized so you may want to find a fairly close example project.</p> </li> <li> <p>How many contributors? If you are working on a team, you‚Äôll want to invest in testing, continuous integration and deployment to make sure the site doesn‚Äôt break. You‚Äôll also want to decide on an editorial process (for an example see <a href="https://github.com/programminghistorian/ph-submissions">the submissions process for the Programming Historian</a>).</p> </li> <li> <p>How much do you want to customize your site? Some static site generators like Gatsby are easier to customize the styling, whereas others have a whole host of themes to choose from but are harder to manipulate.</p> </li> </ol>]]></content><author><name></name></author><summary type="html"><![CDATA[Thinking about static sites beyond individual libraries]]></summary></entry><entry><title type="html">Adding Search to Jekyll Sites with Lunr</title><link href="https://zoeleblanc.github.io/blog/2017/adding-search-to-jekyll-sites-with-lunr/" rel="alternate" type="text/html" title="Adding Search to Jekyll Sites with Lunr"/><published>2017-11-08T00:00:00+00:00</published><updated>2017-11-08T00:00:00+00:00</updated><id>https://zoeleblanc.github.io/blog/2017/adding-search-to-jekyll-sites-with-lunr</id><content type="html" xml:base="https://zoeleblanc.github.io/blog/2017/adding-search-to-jekyll-sites-with-lunr/"><![CDATA[<p>I‚Äôve officially started at the Scholars‚Äô Lab at the University of Virginia as a Digital Humanities Developer, and one of my first tasks was getting search working on their Jekyll static sites. Static sites are generally a great option for most DH projects since they don‚Äôt have many users or dynamic elements, but we often still want some form of search on these sites. Previously the Scholars‚Äô Lab had been using <a href="https://lucene.apache.org/solr/">Solr</a> with their Wordpress engine, but we needed a new solution for a static site projects. While I played around with <a href="https://blog.omc.io/elasticsearch-for-jekyll-part-1-ab456ac7c093">Elasticsearch</a> and custom google searches, I eventually settled on <a href="https://lunrjs.com/">Lunr</a> , which is billed as ‚ÄúA bit like Solr, but much smaller and not as bright‚Äù.</p> <p>Lunr has been the <em>go to</em> search engine for static sites for a few years but there‚Äôs been a lot of recent changes to the code base, which means most of the available tutorials are out of date. So here‚Äôs a brief tutorial outlining everything I had to learn the hard way to get Lunr up and running. You can see a functioning version of the search on this site, and hopefully eventually I can link to some Scholars‚Äô Lab examples of search on different projects. Most of the code in this post was pieced together from trial and error, the <a href="https://github.com/olivernn/lunr.js">Lunr github repo</a>, and the creator of Lunr‚Äôs demo project, <a href="https://github.com/olivernn/moonwalkers">Moonwalkers</a>.</p> <p><strong>So first what is Lunr?</strong></p> <p>Lunr is an inverted index that allows you to build a fast search engine for any pages on your static site. I‚Äôm still learning about the mechanics of search engines, but there are two general types of indexes for searching - forward and inverted. Forward indexes search for terms in a series of documents, whereas inverted indexes search for a list of words and where they appear.</p> <p>A great example is from <a href="https://stackoverflow.com/questions/7727686/whats-the-difference-between-an-inverted-index-and-a-plain-old-index">this Stack Overflow post</a> :</p> <p><em>‚ÄúThe index in the back of a book is actually an inverted index, as defined by the examples above - a list of words, and where to find them in the book. In a book, the table of contents is like a forward index: it‚Äôs a list of documents (chapters) which the book contains, except instead of listing the words in those sections, the table of contents just gives a name/general description of what‚Äôs contained in those documents (chapters).‚Äù</em></p> <p>So Lunr essentially takes whatever you specify and creates an index of the position(s) of each word. Then when you query the index it finds which documents contain that word, and scores the document based on its similarity to the query. Lunr also supports wildcards and boosts, as well as searching on specified fields. You can read more about it‚Äôs functionality and see some examples in the <a href="https://lunrjs.com/guides/searching.html">docs</a>.</p> <p>Now one of the biggest changes to the Lunr codebase is that the index is now an immutable data structure. This means that any updates to the index require recreating the index from scratch. Overall this change is an improvement for maintaining the integrity of the index, but it means that many of the older tutorials are outdated because they are premised on dynamically updating the index.</p> <p>So today I‚Äôm going to outline how you can take advantage of having an immutable and inverted Lunr search through pre-building your index and then querying it dynamically with your Jekyll site. You can see all the code from my own site at <a href="https://github.com/ZoeLeBlanc/zoeleblanc.github.io">my Github repo</a> .</p> <p><strong>So let‚Äôs start building your search engine!</strong></p> <p>First things first, let‚Äôs add some dependencies.</p> <p>Add these gems to your Gemfile.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gem 'json'
gem 'rake'
gem 'front_matter_parser'
</code></pre></div></div> <p>then run <code class="language-plaintext highlighter-rouge">bundle install</code></p> <p>Now either run <code class="language-plaintext highlighter-rouge">npm init</code> or create a package.json. Then run <code class="language-plaintext highlighter-rouge">npm install lunr</code>, <code class="language-plaintext highlighter-rouge">npm install jquery</code>, and if you‚Äôre using dates <code class="language-plaintext highlighter-rouge">npm install moment</code></p> <p><strong>Now let‚Äôs create some automated tasks to compile our corpus and search index</strong></p> <p>[AN: I‚Äôm assuming you don‚Äôt already have a JSON file with all your posts and pages. If you do feel free to skip down to the scripts.]</p> <p>Open your Rakefile or create one if you don‚Äôt have one in the main directory of your site. In your Rakefile, we‚Äôre going to create a task to take all your desired inputs and create a corpus with them.</p> <p>At the top of your Rakefile make sure to require the necessary packages.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>require 'rake'
require 'json'
require 'front_matter_parser'
require 'open3'
</code></pre></div></div> <p>Then let‚Äôs create your first rake task.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>desc "Create corpus for search"
file './corpus.json' =&gt; ['./', *Rake::FileList['_posts/*.md'].exclude()] do |md_file|
     unsafe_loader = -&gt;(string) { YAML.load(string) } #required by front matter parser. Read more at the githu brepo
     corpus = md_file.sources.grep(/\.md$/)
     .map do |path|
        file_path = './' + path
        parsed = FrontMatterParser::Parser.parse_file(file_path, loader: unsafe_loader)
         {
            id: path.pathmap('%n'),
            name: parsed['title'],
            url: parsed['title'].downcase.strip.gsub(' ', '-'),
            content: parsed.content,
         }
     end
     File.open(md_file.name, 'w') do |f|
        f &lt;&lt; JSON.generate(corpus)
     end
end
</code></pre></div></div> <p>So this task starts with a description, <em>creating a corpus for search</em>. Then we list the file we‚Äôre creating, <code class="language-plaintext highlighter-rouge">corpus.json</code> (you can call yours whatever you like but be sure to change it everywhere it‚Äôs referenced). Then we pass in the files we want to use for the corpus. Right now I‚Äôm just passing in the posts, but you could pass in all the pages to the filelist. You can also exclude certain files. For example:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>*Rake::FileList['_posts/*.md', '_projects/*.md'].exclude('1project.markdown')]
</code></pre></div></div> <p>At the end of the <code class="language-plaintext highlighter-rouge">do</code> statement we specify that we‚Äôre inputting markdown files with <code class="language-plaintext highlighter-rouge">md_file</code>. Then we start mapping each file and passing it to the front matter parser gem.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>file_path = './' + path
 parsed = FrontMatterParser::Parser.parse_file(file_path, loader: unsafe_loader)
 {
    id: path.pathmap('%n'),
    name: parsed['title'],
    url: parsed['title'].downcase.strip.gsub(' ', '-'),
    content: parsed.content,
 }
</code></pre></div></div> <p>In the object you specify which properties you want for the corpus and your search index. The front matter parser gem will be able to parse any front matter property. One thing I would recommending is even if you change the properties, be sure to include the url for the search functionality since it‚Äôs how people can click on a post. Finally we write the file to JSON and now you should have a <code class="language-plaintext highlighter-rouge">corpus.json</code>.</p> <p>Then comes our second rake task to build the search index.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>file './search_index.json' =&gt; ['./corpus.json'] do |t|
     Open3.popen2('script/build-index') do |stdin, stdout, wt|
        IO.copy_stream(t.source, stdin)
        stdin.close
        IO.copy_stream(stdout, t.name)
     end
end
</code></pre></div></div> <p>In this task we create <code class="language-plaintext highlighter-rouge">search_index.json</code> from our <code class="language-plaintext highlighter-rouge">corpus.json</code> and a script we have yet to create, called <code class="language-plaintext highlighter-rouge">build-index</code>. This task essentially streams the contents of the corpus into the script and outputs the result to the search index.</p> <p>Now at the bottom of your Rakefile, add these two tasks.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>task :default =&gt; ['./corpus.json', './search_index.json']
</code></pre></div></div> <p><strong>Now we need to create the build-index script.</strong></p> <p>In your script folder, create a file called <code class="language-plaintext highlighter-rouge">build-index</code> and paste this code in it.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#!/usr/bin/env node

var lunr = require('lunr'),
 stdin = process.stdin,
 stdout = process.stdout,
 buffer = []

stdin.resume()
stdin.setEncoding('utf8')

stdin.on('data', function (data) {
    buffer.push(data)
})

stdin.on('end', function () {
     var corpus = JSON.parse(buffer.join(''))

     var idx = lunr( (builder) =&gt; {
         builder.ref('id')
         builder.field('name')
         builder.field('url')
         builder.field('content')
         builder.metadataWhitelist = ['position']
         // This is required to provide the position of terms in
         // in the index. Currently position data is opt-in due
         // to the increase in index size required to store all
         // the positions. This is currently not well documented
         // and a better interface may be required to expose builder
         // to consumers.
         // This is the biggest change to the interface over the
         // 0.x and 1.x branches. Documents must be added to the
         // index within builder closure. When builder function completes
         // the index is immutable, no more documents can be added.
         corpus.forEach(function (doc) {
             builder.add(doc)
         }, builder)
     })

    stdout.write(JSON.stringify(idx))
})
</code></pre></div></div> <p>In this file, we‚Äôre calling Lunr and essentially buffering our corpus data through the Lunr constructor, specifying the fields we want to include and then stringifying the output to write to JSON. The output becomes our <code class="language-plaintext highlighter-rouge">search_index.json</code></p> <p>The comments included are from the creator of Lunr. This script is the biggest change to Lunr, since now both the <code class="language-plaintext highlighter-rouge">builder</code> constructor (instead of earlier use of <code class="language-plaintext highlighter-rouge">ref</code>) and the fact that all the documents have to be added where the Lunr instance is created.</p> <p>Now we need to create a <code class="language-plaintext highlighter-rouge">search.js</code> file in our javascript assets, which will call the <code class="language-plaintext highlighter-rouge">search_index.json</code> and allow us to return search results.</p> <p>Paste this code into <code class="language-plaintext highlighter-rouge">search.js</code></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jQuery(function() {

  $.getJSON('/search_index.json', (data, err) =&gt; {
    window.idx = data;
  });

  $.getJSON('/corpus.json', (data, err) =&gt; {
      window.documents = [];
      Object.entries(data).forEach((key, value)=&gt; {
          var doc = {
              'id' : key[1].id,
              'content': key[1].content,
              'name': key[1].name,
              'url': key[1].url,

          };
          window.documents.push(doc);
      });
  });
  // Event when the form is submitted
  $("#site_search").submit((event) =&gt; {
      event.preventDefault();
      var query = $("#search_box").val(); // Get the value for the text field
      window.index = lunr.Index.load(window.idx);
      var results = window.index.search(query); // Get lunr to perform a search
      display_search_results(results); // Hand the results off to be displayed
  });

  var buildSearchResult = (doc) =&gt; {
    var li = document.createElement('li'),
        article = document.createElement('article'),
        header = document.createElement('header'),
        section = document.createElement('section'),
        h2 = document.createElement('h2'),
        a = document.createElement('a'),
        p1 = document.createElement('p')

    a.dataset.field = 'url';
    a.href += '/blog/' + doc.url;
    a.textContent = doc.name;

    p1.dataset.field = 'content';
    p1.textContent = doc.content;
    p1.style.textOverflow = 'ellipsis';
    p1.style.overflow = 'hidden';
    p1.style.whiteSpace = 'nowrap';

    li.appendChild(article);
    article.appendChild(header);
    article.appendChild(section);
    header.appendChild(h2);
    h2.appendChild(a);
    section.appendChild(p1);

    return li;
  }

  function display_search_results(results) {
      var search_results = $("#search_results");
      if (results.length) {
          search_results.empty(); // Clear any old results

          results.forEach(function(result) {
              var item = window.documents.filter(doc =&gt; doc.id === result.ref);
              var li = buildSearchResult(item[0])// Build a snippet of HTML for this result
              Object.keys(result.matchData.metadata).forEach(function (term) {
                  Object.keys(result.matchData.metadata[term]).forEach(function (fieldName) {
                      var field = li.querySelector('[data-field=' + fieldName + ']'),
                      positions = result.matchData.metadata[term][fieldName].position
                      wrapTerms(field, positions);
                  });
              });
              search_results.append(li);
            });
        } else {
            // If there are no results, let the user know.
            search_results.html('&lt;li&gt;No results found.&lt;br/&gt;Please check spelling, spacing, yada...&lt;/li&gt;');
        }
    }

  function wrapTerms(element, matches) {
    var nodeFilter = {
      acceptNode: function (node) {
        if (/^[\t\n\r ]*$/.test(node.nodeValue)) {
          return NodeFilter.FILTER_SKIP
        }
        return NodeFilter.FILTER_ACCEPT
      }
    }
    var index = 0,
        matches = matches.sort(function (a, b) { return a[0] - b[0] }).slice(),
        previousMatch = [-1, -1],
        match = matches.shift(),
        walker
    if (element instanceof Element) {
        walker = document.createTreeWalker(
          element,
          NodeFilter.SHOW_TEXT,
          nodeFilter,
          false
        )
    } else {
        return 'not an element';
    }
    while (node = walker.nextNode()) {
      if (match == undefined) break
      if (match[0] == previousMatch[0]) continue

      var text = node.textContent,
          nodeEndIndex = index + node.length;

      if (match[0] &lt; nodeEndIndex) {
        var range = document.createRange(),
            tag = document.createElement('mark'),
            rangeStart = match[0] - index,
            rangeEnd = rangeStart + match[1];

        tag.dataset.rangeStart = rangeStart
        tag.dataset.rangeEnd = rangeEnd

        range.setStart(node, rangeStart)
        range.setEnd(node, rangeEnd)
        range.surroundContents(tag)

        index = match[0] + match[1]

        // the next node will now actually be the text we just wrapped, so
        // we need to skip it
        walker.nextNode()
        previousMatch = match
        match = matches.shift()
      } else {
        index = nodeEndIndex
      }
    }
  }
});
</code></pre></div></div> <p>The first thing we do in the file is load the <code class="language-plaintext highlighter-rouge">search_index.json</code> and the <code class="language-plaintext highlighter-rouge">corpus.json</code> into global variables. We transform the corpus file into an array with a series of objects. If you‚Äôre not familiar with <code class="language-plaintext highlighter-rouge">window</code> that allows you to access the variables in the browser console, which is helpful for trouble shooting the search index.</p> <p>Next we create an event listener on the search input field. In this code block, we also instantiate the Lunr Index and pass it our search_index data. Then we can pass that index our search queries from the input field. Finally we pass the results to our functions to display the search results.</p> <p>The <code class="language-plaintext highlighter-rouge">display_search_results</code> function takes the results and makes sure there is an actual result. It then empties out the div of any existing results. Then it loops through the results and filters the corpus to get the right post, and passes the post to the <code class="language-plaintext highlighter-rouge">buildSearchResult</code> function. In that function, we create the DOM elements that we‚Äôll be inserting on the page and we set what elements from the corpus we want to visualize. In this case, I‚Äôm showing the name in an anchor tag with the post url, and then a truncated version of the content.</p> <p>That DOM element is then passed to a loop that looks through the metadata of the results to find the exact position of each word in a document and then pass it to the <code class="language-plaintext highlighter-rouge">wrapTerms</code> function. This function deals with the element nodes and positions of each word in the document to wrap the word with the native html mark tag, which highlights the term. Finally the results are append to the page or if there are no results, then a <code class="language-plaintext highlighter-rouge">no results found</code> is appended.</p> <p>The final steps are to create the <code class="language-plaintext highlighter-rouge">search.html</code> page and link the javascript scripts and node modules.</p> <p>Paste this code into your <code class="language-plaintext highlighter-rouge">search.html</code> or on which ever page you want the search to be available.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;br/&gt;&amp;nbsp;
&lt;form action="get" id="site_search"&gt;
&lt;center&gt;
  &lt;input style="font-size:20px;" type="text" id="search_box"&gt;
  &lt;input style="font-size:20px;" type="submit" value="Go!"&gt;
&lt;/center&gt;
&lt;/form&gt;
&lt;br/&gt;&amp;nbsp;
&lt;br/&gt;&amp;nbsp;

&lt;ul id="search_results"&gt;&lt;/ul&gt;
&lt;!-- You can either host the dependencies from cdns or use the node modules --&gt;
&lt;!-- &lt;script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"&gt;&lt;/script&gt;
&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.19.1/moment.min.js" type="text/javascript" charset="utf-8"&gt;&lt;/script&gt;
&lt;script src="https://unpkg.com/lunr/lunr.js" type="text/javascript" charset="utf-8"&gt;&lt;/script&gt; --&gt;

&lt;script src="/node_modules/jquery/dist/jquery.min.js" type="text/javascript" charset="utf-8"&gt;&lt;/script&gt;
&lt;script src="/node_modules/moment/min/moment.min.js" type="text/javascript" charset="utf-8"&gt;&lt;/script&gt;
&lt;script src="/node_modules/lunr/lunr.js" type="text/javascript" charset="utf-8"&gt;&lt;/script&gt;
&lt;script src="/assets/js/search.js" type="text/javascript" charset="utf-8"&gt;&lt;/script&gt;
</code></pre></div></div> <p>Now that you have everything set up, you should be able to run <code class="language-plaintext highlighter-rouge">rake</code> and have your search index built. Then once you build your jekyll site and serve it, you can go to wherever you‚Äôre hosting your search page and put in a query.</p> <p>You can try out the search function on my site and let me know if you have any issues with lunr! Happy searching üïµÔ∏è</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Tutorial about adding Lunr search indexes to Jekyll sites]]></summary></entry><entry><title type="html">Depictions of Decolonization SHAFR 2017</title><link href="https://zoeleblanc.github.io/blog/2017/depictions-of-decolonization-shafr-2017/" rel="alternate" type="text/html" title="Depictions of Decolonization SHAFR 2017"/><published>2017-06-23T00:00:00+00:00</published><updated>2017-06-23T00:00:00+00:00</updated><id>https://zoeleblanc.github.io/blog/2017/depictions-of-decolonization-shafr-2017</id><content type="html" xml:base="https://zoeleblanc.github.io/blog/2017/depictions-of-decolonization-shafr-2017/"><![CDATA[<p>For SHAFR 2016, I co-organized with Micki Kaufman the first digital history panel at the conference. Our panel was a lot of fun, and we published abbreviated versions of our talks in SHAFR‚Äôs newsletter Passport.</p> <p>This past year (2017), I wasn‚Äôt sure I was going to travel to SHAFR but I was asked to join another digital history panel, and agreed to present some preliminary research avenues. The panel was titled, ‚ÄúDoing Digital History‚Äù, and Micki and I both presented again with Marc Selverstone providing excellent commentary.</p> <p>I don‚Äôt usually post my conference talks, but I wanted to share this one for other scholars who might be interested in using digital image analysis in their studies of public diplomacy.</p> <hr/> <p><img src="/assets/img/Slide01.jpg" alt="slide1"/></p> <p>Depictions of Decolonization: Cairo, Anti-Colonialism, and Digital Image Analysis</p> <p>Good afternoon everyone. Before I get started I want to thank Marc for agreeing to chair this panel and to Micki for helping bring this panel together. Last year we had a great discussion at our ‚ÄúDoing Digital Diplomatic History‚Äù panel at SHAFR and I‚Äôm excited to continue that conversation today.</p> <p><img src="/assets/img/Slide02.jpg" alt="slide2"/></p> <p>Last June, I presented on using digital history to analyze diplomatic dispatches from the American embassy in Cairo in the 1950s and 60s. I‚Äôm still working on that part of my research, but today I‚Äôm shifting gears away from textual sources towards print images. One of the most frustrating aspects of current text analysis methods is that as part of the data cleaning process you often remove the images from the document. While this approach is fine for a study of hundreds of novels, for historical magazines and newspapers this method throws out some of the most interesting historical evidence. So today, I want to share how I‚Äôve worked to integrate images into my digital history methods and research, and hopefully provide some insight into these new research directions for historians interested in studying images in foreign relations.</p> <p><img src="/assets/img/Slide03.jpg" alt="slide3"/></p> <p>So for some background, my dissertation explores Cairo in the 1950s and 60s as a hub for international anti-colonial media production. I trace how Cairo, more so than any other Third World capital, sought to leverage various medias to construct international anti-colonial solidarities. One of the more famous efforts was Radio Cairo, which was broadcast across the continent in multiple languages. And in this slide you can see their broadcast schedule from the summer of 1964.</p> <p>The other arena where Cairo was pre-eminent was as a publication hub with the government funding numerous newspapers and magazines, as well as offering anti-colonial activists from other countries resources to print materials, all of which circulated across the anti-colonial world. In my research, I explore these sources, as well as newspapers from other Third World capitals to understand how the meanings of anti-colonialism shifted over this period. The other image in this slide is from one of these magazines, The Arab Observer, which is the publication I‚Äôm focusing on today, and I think this image is great representation of my dissertation.</p> <p><img src="/assets/img/Slide04.jpg" alt="slide4"/></p> <p>So obviously many diplomatic historians have already studied imagery and foreign relations whether through the lens of culture and diplomatic history, or more recently in studies of public diplomacy and propaganda. While there are large literatures on this subject, I want to mention two recently published books: Sonke Kunkel‚Äôs <em>Empire of Pictures *and Jason Parker‚Äôs *Heart, Minds, Voices</em>. Empire of Pictures explores the ‚Äúglobal flood of images‚Äù that emerged in the 1960s, focusing particularly on the use of images in constructing American empire and power abroad. And a lot of my thinking on this subject draws heavily from Kunkel‚Äôs methodology, and especially his emphasis on pictures as historical actors. Parker‚Äôs *Heart, Minds, Voices *also provides an excellent overview of USIA attempts to counter Third World public diplomacy and to shift international public opinion. Both of these works underscore the importance of media as a political battleground, but the lens of public diplomacy really hasn‚Äôt fully been utilized in studies of non-Western foreign relations. So I‚Äôm trying bridge these studies of public diplomacies with literatures on non-Western media cultures.</p> <p>The United Arab Republic under Gamal Abdel Nasser is a particularly interesting case study since most of the media was either state owned or censored. Thus, many of these publications offer a window into the Egyptian state‚Äôs prescriptive vision of anti-colonialism, especially in the absence of open government archives.</p> <p><img src="/assets/img/Slide05.jpg" alt="slide5"/></p> <p>I first became interested in the symbolism of anti-colonialism in my research on the response to the Congo Crisis in Cairo. While most of my research involved diplomatic cables and speeches at the United Nations, I was surprised to keep finding pictures of Patrice Lumumba‚Äôs widow and children in various Egyptian newspapers and magazines in the early 1960s. I started wondering how these types of pictures were used to symbolize the UAR‚Äôs vision for anti-colonial solidarities.</p> <p><img src="/assets/img/Slide06.jpg" alt="slide6"/></p> <p>A more famous image of these anti-colonial solidarities is this image of Nasser, Nehru, and Tito from the Brioni meeting in July 1956, which many mark as the beginning of the non-aligned movement. These types of pictures often become book covers, but what about their role in shaping this history?</p> <p><img src="/assets/img/Slide07.jpg" alt="slide7"/></p> <p>For a quick tongue and cheek answer, one just has to check Twitter to see what has now become a genre of images of Trump being compared to Obama in what are generally termed ‚Äòphoto-ops‚Äô. As a Canadian, I chose these two, and in many ways these images speak for themselves. Yet, I also want to share these images as a way to consider the ways in which pictures can define political narratives and moments.</p> <p>But today I also want us to consider this same question but with a thousand or tens of thousands of these images. This proposition might feel overwhelming or exciting depending on your perspective, but regardless this hypothetical is increasingly becoming a reality. So how should diplomatic historians proceed?</p> <p>The answer to that question is one that no historian along can provide, but today I‚Äôll try and discuss some of these exciting new research avenues, along with some of the frustrations and ethical grey areas of deploying computer vision.</p> <iframe src="https://giphy.com/embed/zaezT79s3Ng7C" width="480" height="271" frameborder="0" class="giphy-embed" allowfullscreen=""></iframe> <p><a href="https://giphy.com/gifs/liz-lemon-lets-do-this-zaezT79s3Ng7C">via GIPHY</a></p> <p><img src="/assets/img/Slide09.jpg" alt="slide9"/></p> <p>The first step in any digital history project is getting your data. This activity is activity is critical to any digital history project, and most of the existing research on computer vision relies on either born digital images from social media or from previously digitized corpuses, like the Chronicling America project from the Library of Congress. For most historians, these two categories cover a lot of temporal ground. However, for modern historians we‚Äôre stuck between copyright law that‚Äôs slowly inching up the 1920s and digital archives that really only emerged in the late 1990s. This reality means that there‚Äôs limited existing publicly available datasets for the mid-twentieth century. One of the biggest struggles in my research has been finding efficient ways to create my own image collections. However, relying on previously created datasets isn‚Äôt without its own pitfalls. For historians, collecting archival sources is as much a part of our research as writing, and this reality remains true even if you‚Äôre working with digital images.</p> <p>Some existing image management tools include flickr, photo libraries, DevonThink, and various cloud storage offerings. Most of these offerings provide limited means to transform images. This past May, the Roy Rosenzweig Center for History and New Media at George Mason University released Tropy (NOTE: https://tropy.org/), which is probably the best photo management tool I‚Äôve come across. If Tropy had been available earlier, I might have used it more in my research. But instead like the ambitious grad student I am, I actually built my own application that I‚Äôm calling Image Lucida to be my dream research collection platform. That being said, a lot of what I do with Image Lucida can be achieved with freely available sources.</p> <p>My process involves uploading my archival images, organizing them into the relevant folders and adding meta-data, like publication issue, page number, and tags. One of the most difficult steps in this process beyond building the application is creating a standardized ontology for organizing and tagging these images. This cleaning and curation of these sources is the most time consuming, but I believe is a critical step in the research process. Afterall, a different historian interested in different questions might tag certain images differently or organizing images in different hierarchies.</p> <p><img src="/assets/img/Plots_of_covers.gif" alt="slide10"/></p> <p>Also before I go into how I‚Äôve used digital image analysis in my research, I first want to explain a bit about how computers understand images. Images, regardless of file type, are actually a set of pixels which can be read by a machine as a series of numbers, usually in a matrix. Most digital image analysis can be divided into three categories: the use of human tagging of images to identify features, analyzing meta-data that was either entered by humans or generated from image files, and lastly computer vision libraries and proprietary databases that are used to analyze pixels and extract image features. Now there‚Äôs a lot of overlap between these categories, but a helpful distinction between these methods is whether a method uses human interpretation as a basis for analysis or computed statistics of pixel groupings.</p> <p><img src="/assets/img/Slide11.jpg" alt="slide11"/></p> <p>After adding the metadata, I utilize a combination of OCR and computer vision resources. The two main ways I use computer vision is to clean my data and analyze it.</p> <p>So for images that I‚Äôve uploaded, I first extract any text using either the Tesseract or Google Vision APIs. Then If the source does have images, I run it through a computer vision algorithm, called Otsu thresholding, that automatically removes the images. This slide is an example of what the algorithm produces from a cover of The Egyptian Gazette. So as you can see it‚Äôs fairly accurate though newspapers are particularly difficult for computer vision algorithms because they are so busy.</p> <p><img src="/assets/img/Slide12.jpg" alt="slide12"/></p> <p>For today, I‚Äôve processed a number of the issues from The Arab Observer: The Non-Aligned Weekly, which was a state-funded periodical published in Cairo from 1960-1966. Over the course of this period, The Arab Observer circulated throughout anti-colonial, leftist, and revolutionary networks. Full disclosure, what I‚Äôm presenting today is fairly preliminary analysis, but I‚Äôll also talk a bit about where I hope to take this research.</p> <p>The first cover here is the first issue of The Arab Observer that I‚Äôve found from June 26 1960 and the second cover is the last issue of the Arab Observer that I have from October 31, 1966. I have almost 300 issues of the magazine in my collection, though I have found catalogues listing the publication up until 1975.</p> <p>Using a combination of tagging, pixel analysis, and the Google Vision API, I have annotated these covers to try and get a sense of how their symbolism changes over this time period.</p> <p><img src="/assets/img/Slide13.jpg" alt="slide13"/></p> <p>The first question I wanted to explore is which places appeared more often in the covers. Overwhelmingly Cairo is the location for the covers, though towards the end of this period the covers became increasingly abstract and thus they did not have a geographic location. Also for some reason this map represents null values as California, which I left as an interesting question.</p> <p><img src="/assets/img/Slide14.jpg" alt="slide14"/></p> <p>As you would assume, Gamal Abdel Nasser appears frequently on the cover (approx. 28 times). Though the range of subjects varies a great deal. One of my favorite series is looking at the covers from around July 23, which is the anniversary of the Free Officer‚Äôs Revolution.</p> <p><img src="/assets/img/Slide15.jpg" alt="slide15"/></p> <p>While I have tried to extract places and people from these images, I have also been interested in how the themes and formats of the covers change over time. As you can see in this slide, Anti-colonialism was initially the overwhelming focus of the magazine, but over this period you can see a shift in coverage towards development and what I‚Äôve termed cultural heritage.</p> <p><img src="/assets/img/Slide16.jpg" alt="slide16"/></p> <p>This shift is also reflected in the format of the covers. For anti-colonial covers, initially the covers utilized photographs of leaders, but starting in 1962 the magazine started to depict leaders with hand drawn portraits. This continued until about 1965 when the magazine started depicting more abstract covers, with the name becoming The Arab Observer and the Scribe. I argue that this shift towards a more literary and cultural heritage focus for the magazine, coincided with a shift in the tenor of anti-colonial politics in Cairo.</p> <p><img src="/assets/img/Slide17.jpg" alt="slide17"/></p> <p>While the magazine still featured some coverage of anti-colonial leaders and conferences in Cairo, it no longer depicted political events through radical cartoons or strident headlines. While this change may be in part due to changing editorial boards, I also argue that it represents shifting meanings of anti-colonialism in Cairo - away from more radical rhetoric and international focus towards a more nationalist approach, focusing on development and cultural heritage as part of this anti-colonial platform.</p> <p><img src="/assets/img/Slide18.jpg" alt="slide18"/></p> <p>Part of my interest in anti-colonialism is exploring how the emotive meanings shift over this period. To that end, I also utilized Google Vision‚Äôs face detection algorithm to try and see if it could detect the ‚Äòemotions‚Äô in the faces in these covers. This approach is not particularly nuanced since the algorithm only detects for joy, sorrow, surprise, and anger and the face detection is far from perfect.</p> <p><img src="/assets/img/Slide19.jpg" alt="slide19"/></p> <p>Nonetheless, this graph offer another feature for analyzing the symbolism of these images.</p> <p><img src="/assets/img/Slide20.jpg" alt="slide20"/></p> <p>Finally returning that initial image of Nasser, Nehru, and Tito, I have also computed the similarity similarity between this image and all the other covers.</p> <p><img src="/assets/img/Slide21.jpg" alt="slide21"/></p> <p>In the future, I hope to use this method to trace how images travel between publications in Cairo, as well as beyond in other Third World coverage of events, which I believe can help us understand the media theory that animated a lot of Third World activism.</p> <p>So what are my next steps? While these computer vision algorithms are great for initial exploratory analysis, where I‚Äôm most interested in pushing further is using the tags and annotations I‚Äôve done of these images as the basis for a supervised machine learning classifier, which can extend my interpretation over a much larger scale of images. I also hope to start bridging my earlier text analysis with these images, calculating how the symbolism in these images supports and diverges from the surrounding text in these magazines and newspapers.</p> <p>Even though my analysis has some ways to go, there are existing projects that utilize a wide range of methods and utilize a number of other available tools. While computer vision is predominantly a field in computer science department, increasingly digital humanists are exploring the application of digital image analysis to humanities research.</p> <p><img src="/assets/img/Slide22.jpg" alt="slide22"/></p> <p>A pioneer has been Lev Manovich and his work with the Cultural Analytics Lab. Some of his more famous work includes the 2009 project ‚ÄúTimeline,‚Äù that visualized 4,535 covers of Time magazine (1923-2009), and the project ‚ÄúSelfie City‚Äù that explores Instagram selfie trends for different cities. (NOTE: http://selfiecity.net/)</p> <p><img src="/assets/img/Slide23.jpg" alt="slide23"/></p> <p>Another large scale project is the Yale Digital Humanities Center‚Äôs Robots Reading Vogue created by Peter Leonard and Lindsay King, which allows students and researchers to use 6TB of data to explore Vogue‚Äôs 100 year publication history. (NOTE: http://dh.library.yale.edu/projects/vogue/about.shtml)</p> <p><img src="/assets/img/Slide24.jpg" alt="slide24"/></p> <p>Other great examples include Matthew Lincoln‚Äôs study into genre diversity of seventeenth-century Dutch painting and printmaking (NOTE: http://matthewlincoln.net/2016/07/13/dh2016-measuring-genre-diversity-in-seventeenth-century-dutch-painting-and-printmaking.html), Thomas Padilla‚Äôs Three Dimensional analysis of the sci-fi magazine IF (NOTE: http://www.thomaspadilla.org/2016/03/02/3dscifi/), and John Resig and Ryan Baumann who have used different tools to analyze image similarity in art museum collections. (NOTE: https://ryanfb.github.io/etc/2015/11/03/finding_near-matches_in_the_rijksmuseum_with_pastec.html &amp; http://journalofdigitalhumanities.org/3-2/using-computer-vision-to-increase-the-research-potential-of-photo-archives-by-john-resig/)</p> <p>As these titles illustrate, much of this research is occurring at the intersection of digital art history and cultural analytics, which makes sense since these researchers are interested in the visual analysis of large datasets. While these projects largely leverage existing datasets, they do provide a great resource for starting to consider the different types of research avenues that digital image analysis can provide.</p> <p>I would highly recommend looking at these projects if you‚Äôre considering trying to use digital image analysis. Digital humanists and social scientists have also been some of the strongest critics of these computer vision and artificial intelligence algorithms and their applications to everything from state surveillance to image searches. Using resources like the Google Vision API in my application raises questions about how ownership of this data, as well as its uses for other algorithms like state surveillance.</p> <p>Ultimately, computer vision is still a relatively new field, and many of the tools available are not as refined as those for text analysis. Nonetheless, the ability to collect and curate thousands of images is increasingly becoming a reality for historians. For my research, integrating digital image analysis has helped further my exploration into the symbolism of anti-colonialism and opens up the opportunity to explore many new questions. For diplomatic historians interested in public diplomacy, images have already proven to be fertile ground, and I believe integrating digital image analysis into those analyses will enable historians to our hypotheses on a much larger scale. However, I also realize that the barrier to entry is not as easy as some textual analysis tools and understanding the output of computer vision algorithms can be difficult. I hope today I have demonstrated some value for these methods and shown the potential of this newer research method.</p> <p><img src="/assets/img/Slide25.jpg" alt="slide25"/></p> <p>If you‚Äôre interested, under the hood of this application I‚Äôm using a combination of three different computer vision and image analysis libraries that are available in python: Scikit-Image, Pillow, and OpenCV. All of my code is available on Github, and I would be happy to discuss more later.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Talk presented on the Doing Digital History panel]]></summary></entry><entry><title type="html">Digital History Workshop</title><link href="https://zoeleblanc.github.io/blog/2016/digital-history-workshop/" rel="alternate" type="text/html" title="Digital History Workshop"/><published>2016-02-12T00:00:00+00:00</published><updated>2016-02-12T00:00:00+00:00</updated><id>https://zoeleblanc.github.io/blog/2016/digital-history-workshop</id><content type="html" xml:base="https://zoeleblanc.github.io/blog/2016/digital-history-workshop/"><![CDATA[<p>At Vanderbilt, our main departmental gathering is the Vanderbilt History Seminar (aka VHS). In recent years, VHS has started funding small mini workshops on a variety of topics. Last year, I presented at one on the concept of rituals of belonging in our research. This fall I proposed and had accepted a set of two workshops on digital history. You can read my proposal [here], but essentially my desire was to start a conversation in the department about how to meaningfully produce and evaluate digital history scholarship. In many ways, the impetus was the AHA‚Äôs recent Guidelines for Evaluating Digital History, but the proposal was also self-interested.</p> <p>Since last May, I‚Äôve been engaging more deeply with the literature on digital history/humanities, after a great Skype call with Micki Kaufman, who encouraged me to make the leap. So far I‚Äôve presented some preliminary research at SHAFR in June, and eventually designed and developed this website in September. But I‚Äôve been a bit hesitant on how to move forward more meaningfully with digital history in my scholarship.</p> <p>In part, I‚Äôve been busy with archival work and dissertation writing. However, as I‚Äôve become more conversant and interested in digital history, I‚Äôve also become increasingly conflicted, and even at times skeptical, over the term/field. I don‚Äôt want to rehash any of the debates over nomenclature here, but at least personally, I‚Äôm still unsure if I see digital history as something similar to oral history (as a new methodology) or something more akin to cultural or social history (as in a new analytic for asking historical questions). I know it doesn‚Äôt have to be one or the other, but the question boils down to whether one is a digital historian or whether every historian should be using some form of digital [insert tool/method]. While I realize we won‚Äôt solve this dilemma in the workshop, I‚Äôm hoping that the workshop produces some new understandings about what constitutes digital history.</p> <p>This week I‚Äôve been going back over blog posts, articles, and digital projects to create a list of materials for the workshop. Somehow I missed it when he initially posted it, but I found Cameron Blevins‚Äô blog post <a href="http://www.cameronblevins.org/posts/perpetual-sunrise-methodology/">‚ÄúThe Perpetual Sunrise of Methodology‚Äù</a>. and wow he really hits the nail on the head. I encourage everyone to read it, but I also wanted to post this particular section that really resonated with me.</p> <blockquote> <p>‚ÄúBut there is one area in which digital history has lagged behind: academic scholarship.¬†To be clear: I‚Äôm intentionally using ‚Äúacademic scholarship‚Äù in its traditional, hidebound sense of marshaling evidence to make original, explicit arguments. This is¬†an artificial distinction¬†in obvious ways. One of digital history‚Äôs major contributions has, in fact, been to expand the disciplinary definition of scholarship to include things like databases, tools, and archival projects.¬†The scholarship tent has gotten bigger, and that‚Äôs a good thing.¬†Nevertheless¬†there is still an important place inside that tent for using digital methods specifically to advance scholarly claims and arguments about the past.</p> </blockquote> <blockquote> <p>In¬†terms of argument-driven scholarship,¬†digital history has over-promised and under-delivered.¬†It‚Äôs not that historians aren‚Äôt using digital tools to make new arguments about the past. It‚Äôs that there is a fundamental imbalance between the proliferation of digital history workshops, courses, grants, institutes, centers, and labs over the past decade, and the impact this has had in terms of generating scholarly claims and interpretations. The digital wave has crashed¬†headlong¬†into many¬†corners¬†of the discipline. Argument-driven scholarship has largely not been one of them.‚Äù</p> </blockquote> <p>The rest of the post is great too, but when this passage really made me want to high five my screen and shout YES! All my protean frustrations I realize that digital history doesn‚Äôt just have to be about scholarship, but I‚Äôm excited to see that others are both identifying this gap, and producing examples of great digital historical scholarship.</p> <p>Our first workshop is this morning, and beneath the excitement and nerves, I‚Äôm hoping our workshop can start a discussion of how historians can engage more directly with digital history projects. I hope we also help people start thinking about how their research questions might develop through a digital history lens. These goals are a tall order for a workshop, but already I‚Äôve been pleasantly surprised at the amount of interest. If you‚Äôre at Vandy and interested in attending, feel free to stop by (we still have a few more seats) and you can check out details on our website <a href="https://vhsdhist.wordpress.com/">vhsdist.wordpress.com</a>. I‚Äôll try and write about the experience of the first workshop over the weekend.</p> <p>Also this morning, just found that the latest American Historical Review has a whole roundtable about <a href="http://ahr.oxfordjournals.org.proxy.library.vanderbilt.edu/content/121/1.toc">Digital History</a>. Talk about timing!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Blog post about the digital history workshop I organized at Vanderbilt Spring 2016]]></summary></entry><entry><title type="html">Big Data and the Cold War</title><link href="https://zoeleblanc.github.io/blog/2015/big-data-and-the-cold-war/" rel="alternate" type="text/html" title="Big Data and the Cold War"/><published>2015-09-14T00:00:00+00:00</published><updated>2015-09-14T00:00:00+00:00</updated><id>https://zoeleblanc.github.io/blog/2015/big-data-and-the-cold-war</id><content type="html" xml:base="https://zoeleblanc.github.io/blog/2015/big-data-and-the-cold-war/"><![CDATA[<p><a name="backtotop"></a> Big Data and the Cold War are two things that seem like they should go together. Both are big unwieldy entities. And both seem to drive scholars a bit batty. But with the exception of a few digital history projects, I haven‚Äôt really ever come across the two together.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p> <p>This all changed the past month, when big data and Cold War history did sort of come together for me as I was able to attend two separate conferences on each topic.[^2] Even though the two conferences were quite different in format and subject, I wanted to write and think about them together - kind of a forced engagement - even if it‚Äôs just in my head, and now on my website. I‚Äôve written down some of the major questions/problems/shifts that were discussed at these conferences, but what follows is in no way a faithful representation of these conferences (sorry, you‚Äôll have to try and attend them next year). Rather, think of this post as my conference doodles come to life - incoherent and shoddily sketched, but hopefully the beginning of something much more.</p> <p>I still wanted to write a bit about each conference so I‚Äôve created little subheadings. Feel free to jump ahead if you just want my ideas on the intersections between big data and Cold War history.</p> <blockquote> <h4 id="jump-to-making-big-data-human--european-summer-school-on-cold-war-history--big-data-and-cold-war-history">Jump to <a href="#makingbigdatahuman">Making Big Data Human</a> | <a href="#esscwh">European Summer School on Cold War History</a> | <a href="#bigdatacoldwar">Big Data and Cold War history</a></h4> </blockquote> <hr/> <p><a name="makingbigdatahuman"></a> The first conference was one I found on <a href="https://twitter.com/dhiptweets/status/637969086270599169">twitter</a>, called Making Big Data Human. [^2]</p> <p><img src="https://zoeleblanc.github.io/img/post-assets/makingbigdatahuman.jpg" alt="View from the conference"/></p> <p>Luckily, someone canceled last minute so I was able to attend as a participant and head to Cambridge for the first time. The conference was hosted by a great group called <a href="http://doinghistoryinpublic.org">Doing History in Public</a>. The conference was a day long and covered a whole host of topics - from web archiving to database and keyword searches to human geography and visualization. Marta Musso storified the twitter exchange <a href="https://storify.com/martamusso/making-big-data-human">here</a>, which gives a good live feed of the conference, as well as some of my thoughts that I won‚Äôt rehash here.</p> <p>So here are the (my) big thoughts from the conference:</p> <p><strong>1. How do we deal with inconsistency and uncertainty in big data?</strong></p> <p>In the opening keynote, Jane Winters talked about the problems inherent to the patchiness of data collection, especially with respect to web archives. How do we know what‚Äôs missing in massive data set or the degree of accuracy in the data collection? Compounding this difficulty is the reliance of big data on keyword searching, algorithms, and curated web archives, which can be problematic when you consider how these entities may fail to show the gaps in data and flatten complexities though the presentation of materials (i.e. think a list of google search hits versus searching in physical archives). I don‚Äôt think this problem is actually an obstacle, but rather an opportunity to reconsider how we think and talk about our data in the humanities, allowing for some space between object and interpretation.</p> <p>We also had a great discussion about the limitations of databases and queries. I think these discussions are signs of a growing scholarly critique of how data has been and continues to be collected and organized. Yet, we still have a long way to go for how we incorporate the reality of the patchiness of data into our scholarly research and interpretations. For example, we all know archives are abstractions but more often than not we hide many of the limitations of our studies in the footnotes. This approach is less productive in the context of big data, which requires fairly explicit discussion regarding the dataset‚Äôs specific parameters. Hopefully, the broader scope of evidence available through ‚Äúbig data‚Äù can help historians become a bit more honest about their data and analyses.</p> <p><strong>2. How do we identity and work with institutional structures and overcome institutional obstacles to big data in the humanities?</strong></p> <p>A secondary and largely connected theme was that big data projects require] collaborations across disciplines, career stages, and institutions. The conference was great in actually bringing together scholars from across the sciences and the humanities. But it can be difficult to find collaborators in other departments. Furthermore, collaborating raises questions about the intellectual ownership of these digital projects, particularly for humanists who cannot program. In the UK and the EU more so than in the US, there‚Äôs a tendency to fund large scale big data initiatives, which is great for the scope of the project, but poses challenges regarding the division of labor between a single Primary Investigator and many postdocs and students. How this will be resolved remains to be seen, but clearly some of the best collaborations are happening on campuses were some type of infrastructure exists to encourage and facilitate cross-disciplinary collaborations.</p> <p><strong>3. How do we define big data?</strong></p> <p>Lastly, we often returned to the theme of definitions and nomenclature in big data. How do we define big data? Is it merely a large data set? Or is a set of tools? Or even more profoundly, a new methodological and intellectual approach for the humanities? At the end of the conference, we still hadn‚Äôt come to one definitive answer, and like most things in the humanities, I doubt we ever will (which I actually think is a good thing).</p> <p>Many of the speakers and participants in the conference almost immediately started slipping between big data and digital history, which for me raised questions of the overlap between the two. There was also a fairly strong critique over whether big data was anything new or simply the latest iteration of the computational approach first pioneered in the 60s and 70s. Personally, I believe that the changes in web technology and the ability to store and manipulate data has fundamentally altered the potential for big data and digital history, but placing this shift within the context of the early computing revolution is important for not overly exaggerating our current potential.</p> <p>Connected to this question of definition is also a clear need for some way to evaluate the quality of big data and digital history projects. With the AHA releasing its guidelines this summer, clearly the profession is moving in that direction. However, at the conference, many of the critiques revolved around the absence of applying the same scholarly critiques to big data that are applied in more traditional scholarship. Bridging this gap is going to be tough, but increasingly departments are encouraging students and faculty to experiment with digital projects, which will hopefully lead to a more widespread engagement with big data across the humanities.</p> <hr/> <p><a name="esscwh"></a> With these questions in mind, I jetted off to Rome for the <a href="http://www.lse.ac.uk/IDEAS/Projects/Cold%20War%20Studies/Events/Cold%20War%20Summer%20School/CWSS2015CfP.aspx">European Summer School on Cold War History</a> . Organized by a consortium of European Universities, this year‚Äôs hosts were Universit√† Roma Tre e Universit√† Roma Tor Vergata. At the conference, I presented a paper on the impact of the Congo Crisis on Cairo, which I‚Äôll hopefully blog about at a later date. I unfortunately didn‚Äôt tweet at all during the conference (I‚Äôm not sure anyone did actually), so I don‚Äôt have an overview of all the papers or discussions.</p> <p>The conference itself was at the Societa Geografia Italiana, which is in a gorgeous old italian villa in Rome.</p> <p><img src="/assets/img/romeview1.jpg" alt="View from the conference"/></p> <p><img src="/assets/img/romeview2.jpg" alt="View from the conference"/> View from the Conference</p> <p>Given that the Cold War spanned a huge swath of time, the papers at the conference covered a wide variety of topics. I won‚Äôt cover each one here, but just touch on some broader questions/topics.</p> <p><strong>1. How do we define the Cold War?</strong></p> <p>Quite a bit of ink has been spilt on this question, but I was struck by how little we actually discussed the Cold War as an entity, given that this conference addressed the topic (which I actually think is a positive development, instead of getting stuck in circular arguments). I think what the Cold War means really depends on your particular research question. For my work, the Cold War is both a temporal marker and also a force that limits the space for third world solidarities. For others, the Cold War is a historical object that was constructed or a historical actor that shaped cultural identities. I think this multiplicity is a sign of the richness of the scholarship, but the conference did make me think more deeply about what the Cold War means for my work, and how a Cold War lens can illuminate different dynamics.</p> <p><strong>2. Rise of spatial and intellectual history lenses</strong></p> <p>I was also quite pleased to see a number of scholars using spatial and intellectual history lenses, though often implicitly. I think that with the plethora of sources available to modern historians, we sometimes are a little more lazy with respect to the theoretical and analytical framing of our research. Yet, at the conference many of the papers engaged with these analytics, with a focus on spaces like ports and islands, as well as an emphasis on tracing intellectual histories of expertise and discourses. I‚Äôm always struck by the ‚Äúemergent-ness‚Äù of scholarship around a particular lens. Did everyone just hear about these topics in their graduate seminars, or are we all truly the product of our times? Suppose the answer is probably a bit of both. I hope that this trend continues in part selfishly, because it‚Äôs the type of work I find most interesting, but also because I think it will truly open up new fruitful subfields of scholarship.</p> <p><strong>3. Pushback on transnational as a framework</strong></p> <p>Although the term ‚Äútransnational‚Äù was thrown around rather liberally at the conference, on the whole, most of the papers actually focused on local histories within internationalized dynamics, rather than transnational movements. While some stories are inherently transnational (international organizations for example), I think qualifying and limiting the use of transnational is important. Personally, I often feel that transnational histories tend to skew towards a neoliberal imaging of movement, which often ignores power dynamics and states, as well as ignoring the instrumentality of international solidarities/events for local spaces. In simpler terms, very few topics are truly transnational whereas I think most historians are actually working on local spaces that become internationalized, even if they‚Äôre studying social movements or the movements of goods. So I was quite pleased that even though I was at a conference on something profoundly global like the Cold War, the term transnational was not thrown around as liberally or loosely as I‚Äôve seen in the past.</p> <hr/> <p><a name="bigdatacoldwar"></a></p> <p>So now coming back to this question of how these two incredibly broad topics relate to one another.</p> <p>The most obvious overlap is the corpus of materials from the Cold War, and how big data projects might help us understand these archives. This idea is increasingly becoming a reality with the use of digital cameras. However, unlike 19th century historians, Cold War historians still have to contend with copyright laws. In my research, this reality produces counterintuitive emphasis on declassified government documents over press materials, which are still under copyright law.</p> <p>Big data methods also open the opportunity for truly global histories of the Cold War, using a scale of materials beyond the capacity of one researcher to synthesize. Many of the papers at the ESSCWH used multinational and multilingual sources. However, many of the applications for OCR and text mining privilege romantic languages, which at least for now limits and could potentially skew these types of projects. I‚Äôm still struggling with OCR on Arabic script, which makes it difficult to see large-scale patterns in the Arabic daily newspapers I work with.</p> <p>Excepting these limits, big data provides incredible opportunities to visualize narratives in new ways, as well as to trace patterns over time and space. Ultimately, I think big data will help historians of the Cold War be more honest about the gaps in our data and our archives - an important and necessary shift for historians.</p> <p>I also think that thinking about the Cold War can make us more critical about big data. First and foremost, much of higher education was shaped during the Cold War, especially disciplinary and departmental divisions. Historicizing these divisions is critical for opening up spaces for collaborations. At the moment most historians using big data are either using previously digitized or digitally born materials as the basis for their corpus. However, as OCR and cloud services improve, the potential for big data from text sources will increasingly become a reality. Most humanities scholars, and especially historians, are not equipped with the skills to work with these resources and so finding collaborators in the social and computer sciences is going to become critical.</p> <p>Thinking about these questions together has, for me at least, started the wheels turning about the obstacles of copyright, collaboration, and data collection that I think has prohibited Cold War historians from adopting more of the the digital history/big data methods employed by historians of other time periods. I hope that in the future, digital historians will more clearly illustrate the overlap between these two topics. Until then, I guess I‚Äôll have to keep thinking about them, and hope you do too.</p> <p>-Z</p> <hr/> <p><strong>Jump <a href="#backtotop">back to top</a></strong></p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>When I say a few, I‚Äôm really thinking of two in particular: Micki Kaufman‚Äôs <a href="http://blog.quantifyingkissinger.com">Quantifying Kissinger</a> and Matthew Connelly‚Äôs <a href="http://blog.quantifyingkissinger.com">History Lab</a>. If you know of any please let me know!¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[Blog post about two conferences I attended in fall 2015]]></summary></entry><entry><title type="html">Starting Fresh</title><link href="https://zoeleblanc.github.io/blog/2015/starting-fresh/" rel="alternate" type="text/html" title="Starting Fresh"/><published>2015-09-03T00:00:00+00:00</published><updated>2015-09-03T00:00:00+00:00</updated><id>https://zoeleblanc.github.io/blog/2015/starting-fresh</id><content type="html" xml:base="https://zoeleblanc.github.io/blog/2015/starting-fresh/"><![CDATA[<blockquote> <p>Fall is my favorite time of year.</p> </blockquote> <p>Someone once asked me why and in a freudian slip I admitted that it was because of the first day of school. It was a prett nerdy admission, even for me. But it‚Äôs honestly true.</p> <p>I‚Äôve always loved that excitement when you start off new classes and get to see everyone from their summer adventures. I‚Äôm lucky enough that my chosen career has allowed me to continue to experience the early September new school year magic (and might in part be why academia has such an allure). But for the last three years, I‚Äôve been abroad during September (Rabat, Tel Aviv, and now London). Luckily, London has some nice fall weather so I‚Äôm at least feeling that warm fall coziness.</p> <p>In the spirit of a new academic year and a fresh state, I‚Äôve decided to finally clean up my web presence and get my website off the ground. I‚Äôve realized over the last few years that I‚Äôm actually slightly addicted to building websites (websitophilia?) but not really blogging. In fact, I‚Äôm not really all that comfortable posting anything on the web, whether it‚Äôs twitter, instagram, or reddit.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> However, as much as I enjoy being an internet lurker, I also want to start producing something, even if it‚Äôs not a very good something (we all have to start somewhere right?).</p> <p>If you‚Äôve visited previous iterations of my personal site, you‚Äôll find most of the same material. But under the hood this site is all new. For the last version of my site, I bought a turbo charged wordpress theme.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> But I quickly realized that I don‚Äôt need most of the features like a store front or members only area.</p> <p>I really like the look of <a href="http://lincolnmullen.com">Lincoln Mullen</a> and <a href="http://jasonheppler.org">Jason Heppler‚Äôs</a> sites, so I decided to make the move to Jekyll and Markdown (both of which seem increasingly to be the norm for digital humanists).</p> <p>Already I‚Äôm loving the simplicity of building up the site from the basics of a theme I bought<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>, and if you‚Äôre considering making the change, I would highly recommend it. Afterall, fall is for making fresh starts.</p> <p>-Z</p> <p>UPDATE: just found this post on the Chronicle on <a href="http://chronicle.com/article/How-to-Curate-Your-Digital/151001/">‚ÄúHow to Curate Your Digital Identity as an Academic‚Äù</a>, which was kind of the impetus for this redesign. I‚Äôve also tried to clean up my digital presence and get rid of all the half formed websites I‚Äôve created over the years.</p> <hr/> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>Thinking about writing a post about navigating the public/private divide as a twenty-something professional. I know there‚Äôs lots of posts on this topic, but I still often feel unsure if something is too personal or it‚Äôs ok to be authentic‚Ä¶¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2" role="doc-endnote"> <p>Highly recommend <a href="&quot;http://theme.co/x/&quot;">Theme X by themeco</a> if you‚Äôre looking for a crazily functional wordpress theme.¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3" role="doc-endnote"> <p>While I‚Äôve always wanted to design my website from scratch, I unfortunately don‚Äôt really have the time, so I bought the <a href="&quot;http://adventurethemes.com/demo/writer/jekyll/v1-d-20-2/&quot;">Writer theme</a> from Adventure themes. I would also recommend the free <a href="&quot;http://hyde.getpoole.com/&quot;">Hyde theme</a> and the <a href="&quot;http://demo.krownthemes.com/jackman/&quot;">Jackman theme</a>.¬†<a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[Fall is my favorite time of year.]]></summary></entry></feed>